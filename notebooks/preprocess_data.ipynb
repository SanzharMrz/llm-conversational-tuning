{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters from prod\n",
    "\n",
    "\n",
    "def request_number(message):\n",
    "    words = message.split()\n",
    "    forbidden = [\n",
    "        \"call\",\n",
    "        \"phone\",\n",
    "        \"number\",\n",
    "        \"email\",\n",
    "        \"emails\",\n",
    "        \"exchange\",\n",
    "        \"share\",\n",
    "        \"skype\",\n",
    "        \"whatsapp\",\n",
    "        \"facebook\",\n",
    "        \"leave this app\",\n",
    "        \"talk on the phone\",\n",
    "        \"hear your voice\",\n",
    "        \"your voice\",\n",
    "        \"hear you\",\n",
    "        \"to message me\",\n",
    "        \"to text me at\",\n",
    "    ]\n",
    "    for s in forbidden:\n",
    "        if s in words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def request_date(message):\n",
    "    forbidden = [\n",
    "        \"let's go\",\n",
    "        \"lets go\",\n",
    "        \"let's meet\",\n",
    "        \"lets meet\",\n",
    "        \"let's visit\",\n",
    "        \"let's date\",\n",
    "        \"let's hang out\",\n",
    "        \"meet\",\n",
    "        \"meet?\",\n",
    "        \"Would you like to meet\",\n",
    "        \"meet me\",\n",
    "        \"date\",\n",
    "        \"go out\",\n",
    "        \"go on a date\",\n",
    "        \"date you\",\n",
    "        \"date with you\",\n",
    "        \"offline\",\n",
    "        \"visit\",\n",
    "        \"meet you\",\n",
    "        \"see you\",\n",
    "        \"visit you\",\n",
    "        \"in person\",\n",
    "        \"join me\",\n",
    "        \"hang out with me\",\n",
    "        \"date me\",\n",
    "        \"my place\",\n",
    "        \"can come over\",\n",
    "        \"Let's meet up.\" \"Can we get together?\",\n",
    "        \"face-to-face\",\n",
    "        \"Wanna grab coffee\",\n",
    "        \"grab lunch\",\n",
    "        \"grab dinner?\",\n",
    "        \"Meet for a drink?\" \"Let's catch up in person.\",\n",
    "        \"Want to see each other?\",\n",
    "        \"How about a real-life meetup?\",\n",
    "        \"Are you free to hang out?\",\n",
    "        \"I'd love to meet you in person.\",\n",
    "        \"Shall we plan a meet-up?\",\n",
    "        \"Would you like to meet for real?\",\n",
    "        \"Time for an offline rendezvous?\",\n",
    "        \"Let's take this conversation offline.\",\n",
    "        \"Meeting IRL soon?\",\n",
    "        \"IRL\",\n",
    "        \"Care to meet face-to-face?\" \"Coffee date?\" \"Let's make plans to meet.\",\n",
    "        \"Up for an in-person chat?\",\n",
    "        \"Meeting in the physical world?\",\n",
    "        \"We should connect offline.\",\n",
    "        \"How about a meetup this weekend?\",\n",
    "        \"Let's schedule a real-life encounter.\",\n",
    "        \"I'd enjoy meeting you for real.\",\n",
    "        \"Any chance we could meet?\",\n",
    "        \"Want to meet somewhere?\",\n",
    "        \"Let's arrange a live meeting.\",\n",
    "        \"Meeting over a meal?\",\n",
    "        \"Let's get together outside the digital world.\",\n",
    "        \"I propose an in-person get-together.\",\n",
    "    ]\n",
    "    for s in forbidden:\n",
    "        if s in message:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def has_numbers(inputString) -> bool:\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "def has_roleplay(inputString) -> bool:\n",
    "    return \"*\" in inputString\n",
    "\n",
    "\n",
    "FILTERS_MAP = {\n",
    "    \"has_roleplay\": has_roleplay,\n",
    "    \"has_numbers\": has_numbers,\n",
    "    \"request_date\": request_date,\n",
    "    \"request_number\": request_number,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess EVA.AI filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")\n",
    "eva_ai_path = base_path / \"eva_ai_cleaned\" / \"clean\"\n",
    "\n",
    "train_path, test_path = eva_ai_path / \"train.txt\", eva_ai_path / \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "        dialogs = [d.split(\"\\n\") for d in text.split(\"<|endoftext|>\") if d]\n",
    "        dialogs = [\"\\n\".join([d[0]] + [\"\"] + d[1:]) for d in dialogs]\n",
    "        text = \"<|endoftext|>\".join(dialogs)\n",
    "\n",
    "    text = text.replace(\"<|endoftext|>\", \"<s>\")\n",
    "    dialogs = text.split(\"<s>\")\n",
    "    dialogs = [\"<s>\" + dialog.strip(\" \") for dialog in dialogs if len(dialog) > 100]\n",
    "    for i in range(len(dialogs)):\n",
    "        if dialogs[i][-1] != \"\\n\":\n",
    "            dialogs[i] = dialogs[i] + \"\\n\"\n",
    "\n",
    "    new_dialogues = []\n",
    "    for dialogue in dialogs:\n",
    "        dialogue = dialogue.replace(\"<s>\", \"\")\n",
    "        dialogue = dialogue.split(\"\\n\")\n",
    "        dialogue = [r for r in dialogue if len(r) > 0]\n",
    "        bio_info = dialogue[0]\n",
    "        dialogue = dialogue[1:]\n",
    "        new_dialogue = []\n",
    "        for i, sample in enumerate(dialogue):\n",
    "            sample = sample.split(\":\")\n",
    "            t = sample[0]\n",
    "            r = \":\".join(sample[1:])\n",
    "            t, r = t.strip(), r.strip()\n",
    "            filters_res = {}\n",
    "            for key in FILTERS_MAP:\n",
    "                filters_res[key] = FILTERS_MAP[key](r)\n",
    "            new_dialogue.append({\"speaker\": t, \"text\": r, \"filters\": {\"base_filters\": filters_res}, \"order_number\": i})\n",
    "        new_dialogues.append(\n",
    "            {\n",
    "                \"bio_info\": bio_info,\n",
    "                \"dialogue\": new_dialogue,\n",
    "                \"source\": \"eva_ai\",\n",
    "                \"instruction\": \"\",\n",
    "                \"session_id\": str(uuid.uuid4()),\n",
    "            }\n",
    "        )\n",
    "    return new_dialogues\n",
    "\n",
    "\n",
    "train_data = read_data(train_path)\n",
    "test_data = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206 906\n",
      "172 170\n"
     ]
    }
   ],
   "source": [
    "def filtered_data(data):\n",
    "    filtered_data = []\n",
    "    for sample in data:\n",
    "        counter = 0\n",
    "        for reply in sample[\"dialogue\"]:\n",
    "            flag = False\n",
    "            for k in reply[\"filters\"][\"base_filters\"]:\n",
    "                flag = flag or reply[\"filters\"][\"base_filters\"][k]\n",
    "            if flag:\n",
    "                counter += 1\n",
    "        if counter < len(sample[\"dialogue\"]) // 2:\n",
    "            filtered_data.append(sample)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "filtered_train_data, filtered_test_data = filtered_data(train_data), filtered_data(test_data)\n",
    "print(len(train_data), len(filtered_train_data))\n",
    "print(len(test_data), len(filtered_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = base_path / \"eva_ai_cleaned\" / \"json_clean\"\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    os.mkdir(json_path)\n",
    "\n",
    "dump_data(filtered_train_data, json_path / \"train.json\")\n",
    "dump_data(filtered_train_data, json_path / \"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Once dialogues dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")\n",
    "once_path = base_path / \"once_dialogues\" / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_excel(once_path / \"Once_dualogues_p_1.xlsx\")\n",
    "data_2 = pd.read_excel(once_path / \"Once_dualogues_p_2.xlsx\")\n",
    "\n",
    "data_merged = data_1.to_dict(orient=\"records\")\n",
    "data_merged.extend(data_2.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dialogues(data):\n",
    "    # unique_sess_ids = list(set([t['session_id'] for t in data]))\n",
    "    new_data = {}\n",
    "    for sample in data:\n",
    "        sample[\"session_id\"] = str(sample[\"session_id\"])\n",
    "        if sample[\"session_id\"] in new_data:\n",
    "            new_data[sample[\"session_id\"]].append(sample)\n",
    "        else:\n",
    "            new_data[sample[\"session_id\"]] = [\n",
    "                sample,\n",
    "            ]\n",
    "\n",
    "    for key in new_data:\n",
    "        new_data[key] = sorted(new_data[key], key=lambda x: x[\"order_number\"])\n",
    "    return list(new_data.items())\n",
    "\n",
    "\n",
    "train_dialogues = data_to_dialogues(data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data):\n",
    "    new_data = []\n",
    "    for sess_id, dialogue in data:\n",
    "        sample = {}\n",
    "        new_d = []\n",
    "        speaker_map = {\n",
    "            \"Animator\": \"Alice\",\n",
    "            \"User\": \"David\",\n",
    "        }\n",
    "        for d in dialogue:\n",
    "            if not isinstance(d[\"text\"], str):\n",
    "                d[\"text\"] = \"Hm, i don't know what to say\"\n",
    "            new_s = {\n",
    "                \"speaker\": speaker_map[d[\"speaker\"]],\n",
    "                \"text\": d[\"text\"],\n",
    "                \"filters\": {\n",
    "                    \"base_filters\": {key: FILTERS_MAP[key](d[\"text\"]) for key in FILTERS_MAP},\n",
    "                    \"rank_filters\": {\n",
    "                        \"rank_result\": d[\"rank_result\"] if d[\"rank_result\"] != \"first_message\" else \"ok\",\n",
    "                        \"rules_result\": d[\"rules_result\"],\n",
    "                        \"total_score\": d[\"total_score\"],\n",
    "                    },\n",
    "                },\n",
    "                \"order_number\": d[\"order_number\"],\n",
    "            }\n",
    "            new_d.append(new_s)\n",
    "\n",
    "        sample[\"bio_info\"] = (\n",
    "            \"Alice is charming and beautiful girl. Alice is interested in David. David is looking for relatioships. This is a conversation between David and Alice.\"\n",
    "        )\n",
    "        sample[\"dialogue\"] = new_d\n",
    "        sample[\"source\"] = \"once_dump\"\n",
    "        sample[\"session_id\"] = str(sess_id)\n",
    "        sample[\"instruction\"] = \"\"\n",
    "        new_data.append(sample)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "new_train_data = prepare_dataset(train_dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = base_path / \"once_dialogues\" / \"json_clean\"\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    os.mkdir(json_path)\n",
    "\n",
    "dump_data(new_train_data, json_path / \"train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Once FR dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_path = Path(\"../data\")\n",
    "once_path = base_path / \"once_dialogues_fr\" / \"raw\"\n",
    "\n",
    "forbidden_topics_df = pd.read_excel(once_path / \"Dataset_Once_forbidden_topics_FR.xlsx\")[\n",
    "    [\"Rank\", \"Sender\", \"Text\", \"Topic\"]\n",
    "]\n",
    "once_main_fem_df = pd.read_excel(once_path / \"Dataset_Once_MAIN_FR.xlsx\", sheet_name=\"User FEM\")[\n",
    "    [\"#\", \"Sender\", \"Text\", \"Topic\"]\n",
    "].iloc[:5433]\n",
    "once_main_fem_df[\"#\"] = once_main_fem_df[\"#\"].astype(int)\n",
    "\n",
    "once_main_mal_df = pd.read_excel(once_path / \"Dataset_Once_MAIN_FR.xlsx\", sheet_name=\"User MAL\")[\n",
    "    [\"#\", \"Sender\", \"Text\", \"Topic\"]\n",
    "]\n",
    "once_main_fem_df.columns = [\"Rank\", \"Sender\", \"Text\", \"Topic\"]\n",
    "once_main_mal_df.columns = [\"Rank\", \"Sender\", \"Text\", \"Topic\"]\n",
    "\n",
    "once_main_fem_df[\"Sender\"] = once_main_fem_df[\"Sender\"].map(\n",
    "    {\"User(f)\": \"User(f)\", \"Bot(m)\": \"Bot(m)\", \"Animator (m)\": \"Bot(m)\", \"User (f)\": \"User(f)\"}\n",
    ")\n",
    "\n",
    "forbidden_topics_rec = forbidden_topics_df.to_dict(orient=\"records\")\n",
    "once_main_fem_rec = once_main_fem_df.to_dict(orient=\"records\")\n",
    "once_main_mal_rec = once_main_mal_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def process_sample(sample):\n",
    "    return {\n",
    "        \"speaker\": \"Alice\" if sample[\"Sender\"] in [\"bot\", \"Bot(f)\"] else \"David\",\n",
    "        \"text\": sample[\"Text\"],\n",
    "        \"filters\": {\n",
    "            \"base_filters\": {\n",
    "                \"has_roleplay\": False,\n",
    "                \"has_numbers\": False,\n",
    "                \"request_date\": False,\n",
    "                \"request_number\": False,\n",
    "            }\n",
    "        },\n",
    "        \"order_number\": sample[\"Rank\"] - 1,\n",
    "        \"Topic\": sample[\"Topic\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def process_dataset(data, source):\n",
    "    new_data = []\n",
    "    new_sample = [\n",
    "        process_sample(data[0]),\n",
    "    ]\n",
    "    for sample in data[1:] + [\n",
    "        {\"Rank\": 1, \"Sender\": \"bot\", \"Text\": \"\", \"Topic\": None},\n",
    "    ]:\n",
    "        if sample[\"Rank\"] > 1:\n",
    "            new_sample.append(process_sample(sample))\n",
    "        else:\n",
    "            new_data.append(\n",
    "                {\n",
    "                    \"bio_info\": \"Alice est une fille charmante et belle. Alice s'intéresse à David. David recherche des relations. Il s'agit d'une conversation entre David et Alice.\",\n",
    "                    \"dialogue\": new_sample,\n",
    "                    \"source\": source,\n",
    "                    \"session_id\": str(uuid.uuid4()),\n",
    "                    \"instruction\": \"\",\n",
    "                    \"topic\": new_sample[0][\"Topic\"] if new_sample[0][\"Topic\"] is not None else \"\",\n",
    "                    \"language\": \"fr\",\n",
    "                }\n",
    "            )\n",
    "            new_sample = [\n",
    "                process_sample(sample),\n",
    "            ]\n",
    "\n",
    "    new_data_2 = []\n",
    "    for d in new_data:\n",
    "        new_dialogue = []\n",
    "        for t in d[\"dialogue\"]:\n",
    "            del t[\"Topic\"]\n",
    "            new_dialogue.append(t)\n",
    "        d[\"dialogue\"] = new_dialogue[:]\n",
    "        new_data_2.append(d)\n",
    "\n",
    "    return new_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_topics_processed = process_dataset(forbidden_topics_rec, \"forbidden_topics\")\n",
    "once_main_fem_processed = process_dataset(once_main_fem_rec, \"once_main_fr_fem\")\n",
    "once_main_mal_processed = process_dataset(once_main_mal_rec, \"once_main_fr_mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "json_path = base_path / \"once_dialogues_fr\" / \"json_clean\"\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    os.mkdir(json_path)\n",
    "\n",
    "dump_data(forbidden_topics_processed, json_path / \"train_forbidden_topics.json\")\n",
    "dump_data(once_main_fem_processed, json_path / \"train_once_main_fem.json\")\n",
    "dump_data(once_main_mal_processed, json_path / \"train_once_main_mal.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess forbidden topics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_excel(\"../data/once_forbidden_topics/raw/Once_dataset_Forbidden topics_ENG.xlsx\", sheet_name=\"Chats\")[\n",
    "    [\"Rank\", \"Sender\", \"Text\", \"Topic\"]\n",
    "].dropna(subset=[\"Sender\"])\n",
    "data[\"Rank\"] = data[\"Rank\"].astype(int)\n",
    "\n",
    "forbidden_topics_rec_en = data.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def process_sample(sample):\n",
    "    t = {\n",
    "        \"speaker\": (\n",
    "            \"Alice\"\n",
    "            if sample[\"Sender\"]\n",
    "            in [\n",
    "                \"bot\",\n",
    "            ]\n",
    "            else \"David\"\n",
    "        ),\n",
    "        \"text\": str(sample[\"Text\"]).format(username=\"David\"),\n",
    "        \"filters\": {\n",
    "            \"base_filters\": {\n",
    "                \"has_roleplay\": False,\n",
    "                \"has_numbers\": False,\n",
    "                \"request_date\": False,\n",
    "                \"request_number\": False,\n",
    "            }\n",
    "        },\n",
    "        \"order_number\": sample[\"Rank\"] - 1,\n",
    "        \"Topic\": sample[\"Topic\"],\n",
    "    }\n",
    "    return t\n",
    "\n",
    "\n",
    "def process_dataset(data, source):\n",
    "    new_data = []\n",
    "    new_sample = [\n",
    "        process_sample(data[0]),\n",
    "    ]\n",
    "    for sample in data[1:] + [\n",
    "        {\"Rank\": 1, \"Sender\": \"bot\", \"Text\": \"\", \"Topic\": None},\n",
    "    ]:\n",
    "        if sample[\"Rank\"] > 1:\n",
    "            new_sample.append(process_sample(sample))\n",
    "        else:\n",
    "            topic = \"\"\n",
    "            if new_sample[0][\"Topic\"] is not None and isinstance(new_sample[0][\"Topic\"], str):\n",
    "                topic = new_sample[0][\"Topic\"]\n",
    "            new_data.append(\n",
    "                {\n",
    "                    \"bio_info\": \"Alice is charming and beautiful 22 years old girl. Alice is interested in David. David is looking for relatioships. This is a conversation between Alice and David.\",\n",
    "                    \"dialogue\": new_sample,\n",
    "                    \"source\": source,\n",
    "                    \"session_id\": str(uuid.uuid4()),\n",
    "                    \"instruction\": \"\",\n",
    "                    \"topic\": topic,\n",
    "                    \"language\": \"en\",\n",
    "                }\n",
    "            )\n",
    "            new_sample = [\n",
    "                process_sample(sample),\n",
    "            ]\n",
    "\n",
    "    new_data_2 = []\n",
    "    for d in new_data:\n",
    "        new_dialogue = []\n",
    "        for t in d[\"dialogue\"]:\n",
    "            del t[\"Topic\"]\n",
    "            new_dialogue.append(t)\n",
    "        d[\"dialogue\"] = new_dialogue[:]\n",
    "        new_data_2.append(d)\n",
    "\n",
    "    return new_data_2\n",
    "\n",
    "\n",
    "forbidden_topics_processed = process_dataset(forbidden_topics_rec_en, \"forbidden_topics_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_path = Path(\"../data\")\n",
    "json_path = base_path / \"once_forbidden_topics\" / \"json_clean\"\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    os.mkdir(json_path)\n",
    "\n",
    "dump_data(forbidden_topics_processed, json_path / \"forbidden_topics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forbidden_topics_processed_new = []\n",
    "\n",
    "for sample in forbidden_topics_processed:\n",
    "    new_sample = {\n",
    "        \"bio_info\": sample[\"bio_info\"],\n",
    "        \"source\": sample[\"source\"],\n",
    "        \"session_id\": sample[\"session_id\"],\n",
    "        \"instruction\": sample[\"instruction\"],\n",
    "        \"topic\": sample[\"topic\"],\n",
    "        \"language\": sample[\"language\"],\n",
    "    }\n",
    "    d = []\n",
    "    for i in range(len(sample[\"dialogue\"])):\n",
    "        d.append(\n",
    "            {\n",
    "                \"speaker\": sample[\"dialogue\"][i][\"speaker\"],\n",
    "                \"text\": sample[\"dialogue\"][i][\"text\"],\n",
    "                \"to_train\": True,\n",
    "                \"order_number\": sample[\"dialogue\"][i][\"order_number\"],\n",
    "            }\n",
    "        )\n",
    "        if d[-1][\"speaker\"] == \"Alice\":\n",
    "            if len(d) > 1:\n",
    "                forbidden_topics_processed_new.append(\n",
    "                    {\n",
    "                        \"bio_info\": sample[\"bio_info\"],\n",
    "                        \"dialogue\": d[:],\n",
    "                        \"source\": sample[\"source\"],\n",
    "                        \"session_id\": sample[\"session_id\"],\n",
    "                        \"instruction\": sample[\"instruction\"],\n",
    "                        \"topic\": sample[\"topic\"],\n",
    "                        \"language\": sample[\"language\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "len(forbidden_topics_processed_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_data(forbidden_topics_processed_new, json_path / \"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(model, \"../checkpoints/zephyr_7b_forbidden_topics/checkpoint-432\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/zephyr_7b_forbidden_topics/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/zephyr_7b_forbidden_topics/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "with open(\"../data/once_forbidden_topics/json_clean/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    new_sample = copy.deepcopy(sample)\n",
    "    dialogue = new_sample[\"dialogue\"][:]\n",
    "    for i in range(len(dialogue)):\n",
    "        if dialogue[i][\"speaker\"] == \"David\":\n",
    "            dialogue[i][\"to_train\"] = False\n",
    "    new_data.append(new_sample)\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "dump_data(new_data, \"../data/once_forbidden_topics/json_clean/train_mask_user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eplotnikov/miniconda3/envs/once_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/checkpoint-1260\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "with open(\"../data/once_forbidden_topics/json_clean/forbidden_topics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "unique_topics = list(set([s[\"topic\"] for s in data]))\n",
    "\n",
    "topic_insturction_map = {\n",
    "    \"\": \"\",\n",
    "    \"ask phone number\": \"Alice should not agree to communicate with phone\",\n",
    "    \"call\": \"Alice should not agree to join any type of calls\",\n",
    "    \"offline meeting\": \"Alice should not agree to offline meetings\",\n",
    "    \"social\": \"Alice should not agree to share her social media\",\n",
    "    \"give phone number\": \"Alice should not agree to communicate with phone\",\n",
    "}\n",
    "\n",
    "new_data = []\n",
    "for sample in data:\n",
    "    new_sample = copy.deepcopy(sample)\n",
    "    new_sample[\"instruction\"] = topic_insturction_map[new_sample[\"topic\"]]\n",
    "    new_sample[\"topic\"] = \"\"\n",
    "    new_data.append(new_sample)\n",
    "\n",
    "\n",
    "def dump_data(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "dump_data(new_data, \"../data/once_forbidden_topics/json_clean/train_no_topics_instructions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = \"../checkpoints/mistral_egor_v1/checkpoint-61720\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"/home/eplotnikov/Once/onceLM/checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/checkpoint-180\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v2/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = \"../checkpoints/mistral_egor_v1/checkpoint-61720\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"/home/eplotnikov/Once/onceLM/checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/checkpoint-108\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v3/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = \"../checkpoints/mistral_egor_v1/checkpoint-61720\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"/home/eplotnikov/Once/onceLM/checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/checkpoint-88\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v4/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eplotnikov/miniconda3/envs/once_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# base_model = \"../checkpoints/mistral_egor_v1/checkpoint-61720\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"/home/eplotnikov/Once/onceLM/checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/checkpoint-76\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v5/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eplotnikov/miniconda3/envs/once_train/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/tokenizer_config.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/special_tokens_map.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/tokenizer.model',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/added_tokens.json',\n",
       " '../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_name = \"../checkpoints/mistral_egor_v1/checkpoint-61720\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "model = peft.PeftModel.from_pretrained(\n",
    "    model, \"/home/eplotnikov/Once/onceLM/checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/checkpoint-76\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/mistral_egor_v1/mistral_forbidden_topics_lora_v6/final_checkpoint/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

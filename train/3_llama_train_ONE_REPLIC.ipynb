{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e33f0f0-0e4c-454d-8443-41ed7423447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03b5bcb-466e-48df-9997-0bacedba8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/razant/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.0_stable\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "EXP_NAME = 'llama13b-Leah-short-v3'\n",
    "if not os.path.exists(EXP_NAME):\n",
    "    os.makedirs(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6408767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import math\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29478fa-ef8d-4a5b-8786-84fb9fa20ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "TEST_TXT = \"test_merged.txt\"\n",
    "TRAIN_TXT = \"train_merged.txt\"\n",
    "\n",
    "train_paths = glob.glob('./txt/*.txt')\n",
    "\n",
    "train_txt = ''\n",
    "length = {}\n",
    "for path in train_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('<|endoftext|>', '<s> ')\n",
    "        text = text.replace('\\n\\n','\\n').replace('\\n\\n','\\n')\n",
    "        length[path.split('/')[-1].split('.')[0]] = len(text.split('\\n'))\n",
    "        train_txt += text\n",
    "        \n",
    "        \n",
    "        \n",
    "with open(TRAIN_TXT, 'w') as f:\n",
    "    f.write(train_txt)\n",
    "    \n",
    "    \n",
    "with open(TEST_TXT, 'r') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('<|endoftext|>', '<s> ')\n",
    "    text = text.replace('\\n\\n','\\n').replace('\\n\\n','\\n')\n",
    "    \n",
    "with open(TEST_TXT, 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e6fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "name = 'TheBloke/Llama-2-13B-fp16'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False, legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(name).half().to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a7c1ab-f956-4937-9620-a478f96d9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TXT = \"test_merged.txt\"\n",
    "TRAIN_TXT = \"train_merged.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46300776",
   "metadata": {},
   "source": [
    "# Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090b188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95919cd-61a0-408a-94e2-708e8497b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TRIGGER_SET = set([\"Leah\"])\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "        dialogs = text.split(\"<s> \")\n",
    "        dialogs = [\"<s> \" + dialog.strip(\" \") for dialog in dialogs if len(dialog) > 100]\n",
    "        for i in range(len(dialogs)):\n",
    "            if dialogs[i][-1] != \"\\n\":\n",
    "                dialogs[i] = dialogs[i] + \"\\n\"\n",
    "        self.dialogs_full = dialogs\n",
    "        self.dialogs = []\n",
    "        for dialog in self.dialogs_full:\n",
    "            dialog = dialog.split('\\n')\n",
    "            for i in range(1, len(dialog) - 1):\n",
    "                if dialog[i].split(\":\")[0] in TRIGGER_SET:\n",
    "                    self.dialogs.append('\\n'.join(dialog[0:i+1]) + random.choice([\"\\n\", \"\"]))\n",
    "                    \n",
    "        random.shuffle(self.dialogs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        dialog = self.dialogs[i]\n",
    "        tokens = tokenizer.encode(dialog, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        N = random.choice([1024])\n",
    "        if len(tokens[0]) > N:\n",
    "            dialog_lines = dialog.split(\"\\n\")\n",
    "            prompt = dialog_lines[0:1]\n",
    "            lines = dialog_lines[1:]\n",
    "            idx = random.randint(0, len(lines) - 7)\n",
    "            idx = random.choice([0,idx,idx])\n",
    "            new_lines = lines[idx:idx + 7]\n",
    "            dialog = \"\\n\".join(prompt + new_lines) + random.choice([\"\\n\", \"\"])\n",
    "            tokens = tokenizer.encode(dialog, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        return tokens[0]\n",
    "    \n",
    "    \n",
    "class MyDataset_eval():\n",
    "    def __init__(self, path):\n",
    "        super().__init__\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "        dialogs = text.split(\"<s> \")\n",
    "        dialogs = [\"<s> \" + dialog.strip(\" \") for dialog in dialogs if len(dialog) > 100]\n",
    "        for i in range(len(dialogs)):\n",
    "            if dialogs[i][-1] != \"\\n\":\n",
    "                dialogs[i] = dialogs[i] + \"\\n\"\n",
    "        self.dialogs = dialogs\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        dialog = self.dialogs[i]\n",
    "        tokens = tokenizer.encode(dialog, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        if len(tokens[0]) > 1024:\n",
    "            dialog_lines = dialog.split(\"\\n\")\n",
    "            prompt = dialog_lines[0:1]\n",
    "            lines = dialog_lines[1:]\n",
    "            idx = random.randint(0, len(lines) - 7)\n",
    "            idx = random.choice([0,idx,idx])\n",
    "            new_lines = lines[idx:idx + 7]\n",
    "            dialog = \"\\n\".join(prompt + new_lines)\n",
    "            tokens = tokenizer.encode(dialog, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        return tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8781fe8a-e0b0-4068-9b97-d01e718fc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(TRAIN_TXT)\n",
    "eval_dataset = MyDataset_eval(TEST_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33483102",
   "metadata": {},
   "source": [
    "# Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380987ec-e51c-44be-9d50-9488a6116412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10904a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\": 1,\n",
    "    \"bs\": 1,\n",
    "    \"warmup\": 0.4,\n",
    "    \"grad_accum\": 8,\n",
    "    \"lr\": 1e-5,\n",
    "    \"tokens\": 1024,\n",
    "    \"training\": True,\n",
    "}\n",
    "\n",
    "train_dataset = dataset\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "model.training = args[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6dfb359-0777-48e7-8677-0367e9753b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "  \n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    @staticmethod\n",
    "    def find_last_instance(mylist, symbol):\n",
    "        for i in range(len(mylist) - 1, -1, -1):\n",
    "            if mylist[i] == symbol:\n",
    "                return i + 1\n",
    "        return 0 \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        symbol = 29901\n",
    "        last_idx = self.find_last_instance(list(labels.squeeze(0).cpu().numpy()), symbol)\n",
    "\n",
    "        # Создание маски\n",
    "        mask = torch.zeros(len(labels.squeeze(0))).to(DEVICE)\n",
    "        if last_idx > 0:  # Проверка, найден ли символ\n",
    "            mask[last_idx:] = 1\n",
    "        else:\n",
    "            mask += 1  # Если символ не найден, используется полная маска\n",
    "            print(labels)\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
    "        loss *= mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e3860-c339-4631-94a3-a938f1763967",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c64d8d-7f87-46b9-8f6e-f681c2e4678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_tests import logic_qa_test, logic_multiturn_test, bio_test\n",
    "from transformers import EvalPrediction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "BOT_NAME = 'Leah'\n",
    "prompt = 'This is a chat with Leah, she is a barista from LA'\n",
    "USER_NAME = 'Andrew'\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    bio_acc = bio_test('Portrait — Leah - mini dialogues.tsv', model, tokenizer, prompt, BOT_NAME)\n",
    "    logic_qa_acc = logic_qa_test(\"Logic autotest - 2 lines.tsv\", model, tokenizer, prompt, BOT_NAME)\n",
    "    logic_multiturn_acc = logic_multiturn_test(\"Logic autotest - Sheet1.tsv\", model, tokenizer, prompt, BOT_NAME)\n",
    "    result = {\n",
    "        \"bio\": bio_acc,\n",
    "        \"logic_qa\": logic_qa_acc,\n",
    "        \"logic_multiturn\": logic_multiturn_acc,\n",
    "        \"all\": (bio_acc + logic_qa_acc + logic_multiturn_acc) / 3\n",
    "    }\n",
    "\n",
    "    with open(f'{EXP_NAME}/logs.txt', 'a') as f:\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af0480-d9b7-4f09-bb96-d933a2d25bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 43/65 [03:12<01:42,  4.65s/it]"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"bio_0\": bio_test('Portrait — Leah - mini dialogues.tsv', model, tokenizer, prompt, BOT_NAME),\n",
    "    \"QA_0\": logic_qa_test(\"Logic autotest - 2 lines.tsv\", model, tokenizer, prompt, BOT_NAME),\n",
    "    \"Multiturn_0\": logic_multiturn_test(\"Logic autotest - Sheet1.tsv\", model, tokenizer, prompt, BOT_NAME)}\n",
    "\n",
    "with open(f'{EXP_NAME}/logs.txt', 'a') as f:\n",
    "    f.write(str(result))\n",
    "    f.write('\\n')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04635efd",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757af58-ce5f-4e3c-9109-3668675e9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "         output.requires_grad_(True)\n",
    "\n",
    "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baf6ae-2f13-4862-90cb-1f78f2d7d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze = ['embed_tokens', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'norm', 'lm_head']\n",
    "def freeze(model, unfreeze):\n",
    "    for name, p in model.named_parameters():\n",
    "        name = name.lower()\n",
    "        p.requires_grad = False\n",
    "        for target in unfreeze:\n",
    "            if target in name:\n",
    "                p.requires_grad = True\n",
    "    return model\n",
    "\n",
    "\n",
    "model = freeze(model, unfreeze) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11590e-70c3-4df7-b22a-ad55e181a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics(None)\n",
    "path = f\"./{EXP_NAME}-ckpts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a8ff4-2f4d-4b36-9b6f-0d4b6dcc5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    gradient_checkpointing = True,\n",
    "    output_dir=path,\n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=args[\"epochs\"],\n",
    "    per_device_train_batch_size=args[\"bs\"],\n",
    "    per_device_eval_batch_size=args[\"bs\"],\n",
    "    warmup_ratio=args[\"warmup\"],\n",
    "    gradient_accumulation_steps=args[\"grad_accum\"],\n",
    "    optim=\"adafactor\",\n",
    "    learning_rate=args[\"lr\"],\n",
    "    # use_cache = False,\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    eval_steps = 500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=False,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"bio\",\n",
    "    eval_delay = 0,\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers = (ChildTuningAdam(model.parameters(), lr=args[\"lr\"], reserve_p=args[\"p\"], weight_decay=args[\"wd\"]), None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44843cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "py310_peft",
   "language": "python",
   "name": "py310_peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
